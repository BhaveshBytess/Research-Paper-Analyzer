The paper addresses performance drops in timestamped directed graph neural networks (GNNs) for tasks like user classification and fraud detection, caused by label leakage when adapting undirected models to time-directed inference. It resets baselines to highlight failures of undirected training/inference in directed settings. The main contribution is TRD-GNN, introducing a novel GNN sampling layer that relaxes the time-directed graph structure during training, limited to prevent label leakage while enabling robust deployment. This bridges the mismatch in message passing semantics. Benefits are demonstrated qualitatively on public benchmark and proprietary e-commerce datasets for node classification.