# active_learning/trainer.py
"""
Lightweight trainer harness (optional). Uses Hugging Face transformers + PEFT (LoRA) to fine-tune
a small sequence classification model on the labeled CSV generated by annotator.
This module is *optional* and will only run if dependencies exist and user wants to fine-tune.
"""

from typing import Optional
from pathlib import Path
import logging

def can_train() -> bool:
    try:
        import torch
        import transformers
        import peft
        return True
    except Exception:
        return False

def train_from_csv(csv_path: str,
                   text_column: str = "context_text",
                   label_column: str = "label",
                   model_name: str = "distilbert-base-uncased",
                   output_dir: str = "al_finetune",
                   num_train_epochs: int = 3,
                   per_device_train_batch_size: int = 8,
                   learning_rate: float = 5e-5,
                   use_lora: bool = True) -> Optional[str]:
    """
    Train a small classifier from CSV. This function is intentionally high-level and will:
      - load CSV into a HuggingFace Dataset
      - tokenize with model tokenizer
      - configure Trainer with LoRA via PEFT if use_lora True and peft available
      - train and save weights to output_dir
    NOTE: This will be slow on CPU. GPU recommended. This function checks dependencies and raises informative errors if missing.
    """
    if not can_train():
        raise RuntimeError("Training dependencies not installed (torch/transformers/peft). Install or use mock mode.")

    # Lazy imports (only when training requested)
    import torch
    from datasets import load_dataset
    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
    from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training

    # Load dataset
    ds = load_dataset("csv", data_files=csv_path)["train"]
    # Build label mapping
    labels = sorted(list(set(ds[label_column].tolist())))
    label2id = {l:i for i,l in enumerate(labels)}
    num_labels = len(labels)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    def preprocess(ex):
        t = tokenizer(ex[text_column], truncation=True, padding="max_length", max_length=256)
        t["labels"] = label2id[ex[label_column]]
        return t
    ds = ds.map(preprocess, batched=False)
    ds = ds.train_test_split(test_size=0.1)

    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

    if use_lora:
        # prepare for int8 training if desired (this is optional)
        try:
            model = prepare_model_for_int8_training(model)
        except Exception:
            # continue without int8 prep
            pass
        config = LoraConfig(
            r=8,
            lora_alpha=32,
            target_modules=["q", "v"] , # may need to adjust by model type
            lora_dropout=0.05,
            bias="none",
            task_type="SEQ_CLS"
        )
        model = get_peft_model(model, config)

    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=per_device_train_batch_size,
        num_train_epochs=num_train_epochs,
        learning_rate=learning_rate,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="steps",
        logging_steps=50,
        remove_unused_columns=False,
        fp16=torch.cuda.is_available()
    )

    def collate_fn(batch):
        import torch
        keys = batch[0].keys()
        input_ids = torch.stack([torch.tensor(x["input_ids"]) for x in batch])
        attention_mask = torch.stack([torch.tensor(x["attention_mask"]) for x in batch])
        labels = torch.tensor([x["labels"] for x in batch])
        return {"input_ids": input_ids, "attention_mask": attention_mask, "labels": labels}

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=ds["train"],
        eval_dataset=ds["test"],
        data_collator=collate_fn,
        tokenizer=tokenizer
    )

    trainer.train()
    trainer.save_model(output_dir)
    return output_dir
