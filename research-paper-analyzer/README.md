# Research Paper Analyzer

*Turn long research PDFs into grounded, structured JSON and an evidence-anchored summaryâ€”fast.*

![demo](assets/deepseek.mp4) <!-- Replace with actual GIF later -->

---

## Features

* Deterministic â‰¥0.9 **summary alignment** (no API required); LLM optional for polish
* Supports **Gemini** and **OpenRouter (DeepSeek)** backends, plus a **Mock** mode
* Clean **post-processing, confidence scoring, and sidecar metadata** for auditability

---

## Quickstart (90 seconds)

```bash
# 1) Create & activate venv
python -m venv venv
source venv/bin/activate       # Linux/Mac
.\venv\Scripts\activate        # Windows PowerShell

# 2) Install deps
pip install -r requirements.txt
pip install sentence-transformers rapidfuzz nltk
python -m nltk.downloader punkt

# 3) Configure keys (optional for LLMs)
# Create .env in repo root:
# GEMINI_API_KEY=your_key
# OPENROUTER_API_KEY=your_key

# 4) Run the app
streamlit run app/app.py
```

Or run the pipeline directly on a sample JSON:

```bash
# Post-process
python scripts/postprocess_paper.py examples/example_full.json

# Validate (semantic)
python scripts/validate_summary_semantic.py examples/example_full.clean.json

# Repair deterministically and re-validate
python scripts/repair_summary_anchor_semantic.py examples/example_full.clean.json examples/example_full.repaired.json
python scripts/validate_summary_semantic.py examples/example_full.repaired.json
```

Expected: alignment score â‰¥ 0.9 (sample reaches **1.0** after repair).

---

## What you get

* **Structured extraction**: metadata, methods, results, datasets/tasks, limitations, summary
* **Evidence for every claim**: `{ page, snippet }` collections preserved
* **Confidence**: per-result + aggregate (results mean)
* **Clean outputs**:

  * `paper.clean.json` â€” canonicalized, enriched
  * `paper.meta.json` â€” sidecar `_meta`
  * `paper.repaired.json` â€” summary anchored to evidence

---

## Example Metrics (sample run)

| Metric             | Value |
| ------------------ | ----- |
| JSON validity      | 98%   |
| Evidence precision | 92%   |
| Summary alignment  | 1.0   |

(Values vary by paper; reported for demo JSON)

---

## Architecture (file-wise)

* **Ingestion** â€” `ingestion/parser.py`: PDF â†’ pages/text (born-digital; no OCR yet)
* **Evidence** â€” `evidence/locator.py`: find and store `{page, snippet}`
* **Orchestration** â€” `orchestrator/pipeline.py`, `orchestrator/heads.py` (+ `prompts/*.txt`):

  * Backends: Gemini, OpenRouter (DeepSeek), Mock
  * Heads: metadata, methods, results, summary
* **Schema** â€” `schema/*.py`, `schema/paper.schema.json`: Pydantic + JSON Schema
* **Post-process** â€” `scripts/postprocess_paper.py` + `results/compute_confidence.py`
* **Validate** â€” `scripts/validate_summary_semantic.py` (embeddings + fuzzy)
* **Repair** â€” `scripts/repair_summary_anchor_semantic.py` (deterministic â‰¥0.9)
* **UI** â€” `app/app.py` (Streamlit)
* **Examples** â€” `examples/*.json` (raw/clean/repaired)

ðŸ“‚ Flow: Parse â†’ Extract (LLM) â†’ Merge â†’ Post-process â†’ Validate â†’ Repair â†’ Validate â†’ Serve.

---

## Configuration

* `.env` (optional if using Mock only)

  * `GEMINI_API_KEY` â€” Google Gemini
  * `OPENROUTER_API_KEY` â€” OpenRouter (e.g., `deepseek/deepseek-chat`)

Backend selection happens in the Streamlit UI or in `orchestrator/heads.py`.

---

## Deterministic summary alignment (why it matters)

We measure **summary-to-evidence grounding** per sentence.

* Semantic validator: `sentence-transformers` + cosine, with `rapidfuzz` fallback
* Repair strategy:

  * If sentence matches evidence strongly â†’ keep + append `(see pX: "snippet...")`
  * If weak â†’ replace with best evidence `(pX)`

Guarantees â‰¥0.9 alignment without LLMs; use LLM later for style.

---

## Troubleshooting

* **Activation**:

  * Linux/Mac â†’ `source venv/bin/activate`
  * Windows â†’ `.\venv\Scripts\activate`
* **Scanned PDFs**: OCR not enabled. Add Tesseract/pytesseract before ingestion.
* **Model download slow?** First run only; cached afterward.
* **Alignment low (<0.9)?** Ensure evidence exists â†’ run repair script â†’ adjust threshold (0.70) for paraphrases.

---

## ðŸ“‚ Repo Map (selected)

* `app/app.py` â€” UI
* `ingestion/parser.py` â€” PDF parsing
* `orchestrator/*.py` â€” pipeline, heads, merge
* `prompts/*.txt` â€” head prompts
* `schema/*.py` â€” data models
* `scripts/*.py` â€” post-process, validate, repair
* `examples/*.json` â€” sample outputs

---

## Contributing

* Dev setup: create venv, install deps, run tests
* PRs welcome for:

  * new metrics
  * tasks/datasets lexicons
  * OCR integration
  * additional LLM backends

---

## License

MIT (see LICENSE)


